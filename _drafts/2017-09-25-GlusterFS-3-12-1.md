---
published: false
---

We're in the process of shifting from using our [custom 'glue'][1] for orchestrating Docker deployments to Kubernetes.

Part of this is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications.

While NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur.

I spend some time evaluating various cluster filesystems / network block storage and the two that stood out were Ceph and Gluster.

I settled on Gluster as the most suitable for our needs, it's far less complex to deploy than Ceph, it has less moving pieces and files are stored in a familiar manner on hosts.

## Implementation

I've settled on a 3 node deployment with one node as an [arbiter](http://docs.gluster.org/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/) (replica 3, arbiter 1).

Our nodes are CentOS 7 VMs within our exiting XenServer infrastructure, each node has 8 vCPUs [Xeon E5-2680 v4](https://ark.intel.com/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz), 16GB of RAM and backed by our [iSCSI SSD Storage](https://smcleod.net/tech/ssd-storage-cluster-diagram/).

## Automation / Puppet

## Performance

## Recovery

## Annoyances

## Notes

[1] When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn't any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.