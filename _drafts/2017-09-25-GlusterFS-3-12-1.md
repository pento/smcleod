---
published: false
---

We're in the process of shifting from using our [custom 'glue'][1] for orchestrating Docker deployments to Kubernetes.

Part of this is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications.

While NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur.

I spend some time evaluating various cluster filesystems / network block storage and the two that stood out were Ceph and Gluster.

I settled on Gluster as the most suitable for our needs, it's far less complex to deploy than Ceph, it has less moving pieces and files are stored in a familiar manner on hosts.

## Implementation

I've settled on a 3 node deployment with one node as an [arbiter](http://docs.gluster.org/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/) (replica 3, arbiter 1).

Our nodes are CentOS 7 VMs within our exiting XenServer infrastructure, each node has 8 vCPUs [Xeon E5-2680 v4](https://ark.intel.com/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz), 16GB of RAM and backed by our [iSCSI SSD Storage](https://smcleod.net/tech/ssd-storage-cluster-diagram/).

## Automation / Puppet

We're long time, heavy users of Puppet so naturally I'm deploying Gluster via a [Puppet module](https://github.com/voxpupuli/puppet-gluster) and generating volumes and volume configuration from [Hiera](https://docs.puppet.com/hiera/).

Volumes are automatically generated from the Hiera structure that defines our applications.

{% highlight yaml %}
# profiles::services::gluster::volume:
profiles::services::gluster::volume::pool: "%{alias('profiles::services::gluster::host::pool')}"
profiles::services::gluster::volume::pool_members: "%{alias('profiles::services::gluster::host::pool_members')}"
profiles::services::gluster::volume::brick_mountpoint: '/mnt/gluster-storage'
profiles::services::gluster::volume::replica: 3
profiles::services::gluster::volume::arbiter: 1
profiles::services::gluster::volume::volume_options:
  # Failover clients after 10 seconds of a server being unavailable
  'network.ping-timeout': '10'
  'cluster.lookup-optimize': 'true'
  'cluster.readdir-optimize': 'true'
  'cluster.use-compound-fops': 'true'
  'performance.parallel-readdir': 'true'
  'performance.client-io-threads': 'true'
  'performance.stat-prefetch': 'true'
  'diagnostics.brick-log-level': 'WARNING'
  'diagnostics.client-log-level': 'WARNING'
  'server.event-threads': '3'
  'client.event-threads': '3'
{% endhighlight %}

I enabled the `nis_enabled` SEbool to prevent a number of SELinux denials I noticed in the logs:

{% highlight yaml %}
profiles::os::redhat::centos7::selinux::selinux_booleans:
  'nis_enabled':
    value: 'on'
{% endhighlight %}

I also increased the local emepheral port range and the kernel's socket backlog limit as suggested by Redhat:

{% highlight yaml %}
sysctl::base::values:
  'net.ipv4.tcp_max_syn_backlog':
    ensure: present
    value: '4096'
    comment: 'Increase syn backlogs for gluster'
  'net.ipv4.ip_local_port_range':
    ensure: present
    value: '32768 65535'
    comment: 'Increase local port range for gluster'
  'net.core.somaxconn':
    ensure: present
    value: '2048'
    comment: 'Increase kernel socket backlog limit for gluster'
{% endhighlight %}

## Performance

Performance is, well, very poor for anything other than large reads.

I was expecting a hit to IOP/s performance as you would with any clustered, network file system, but I wasn't expecting it to drop as much as it did, especially after enabling the above performance options on the volumes.

TODO - Insert benchmarks and graphs here.

## Recovery

## Annoyances

Gluster seems to love open file handles, on an average node in the cluster with 120 small volumes connected to 3 fuse clients and no files I often see up to 1.5 _million_ open files:

{% highlight shell %}
root@int-gluster-02:~  # lsof | wc -l
1042782

root@int-gluster-01:~  # netstat -lnp|grep gluster|wc -l
111
{% endhighlight %}


## Notes

[1] When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn't any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.